\section{Measurement}
\label{section:measurement}


\subsection{\bf Experiment Setup}
We set up Hadoop (1.0.4) on 6 nodes which form a two-tier tree topology. We have two machines in each rack and thus have three racks in total. Each machine in a rack connects to a ToR switch and three ToR switches connect to a high-level aggregation switch. The master(JobTracker and NameNode) and Secondary NameNode are put in the same rack. The hostname and IP address used as well as the functionality each machine takes are as follows:
\begin{itemize}
\item {\bf 1st rack:}
\\
frodo (10.10.101.34) (JobTracker and NameNode)

boromir (10.10.101.34) (Secondary NameNode)

\item {\bf 2nd rack:}
\\
gollum (10.10.101.35) (TaskTracker and DataNode)

bilbo (10.10.101.36)	(TaskTracker and DataNode)

\item {\bf 3rd rack:}
\\
pippin (10.10.101.37)	(TaskTracker and DataNode)

aragorn (10.10.101.39)	(TaskTracker and DataNode)
\end{itemize}

\subsection{\bf Workload Description}
We run three kinds of workloads and capture the pcap trace on each machine.
(1) Write files to HDFS
We first run one HDFS client which writes a 1GB file into HDFS on each of the 6 machines at the same time. Each client will write 4KB at one time until it reaches 1GB. HDFS block size is set to 64MB and replica number is 3.

(2) Read
We then run four HDFS clients at the same time, one on each slave node (4 nodes in total). Each client will read the 1GB file we just generated on that slave node. HDFS block size is set to 64MB and replica number is 3.

(3) Change the number of replica on the fly
We then change the number of replicas from 3 to 4 for one 1GB file we just generated in HDFS with command "hadoop dfs -setrep -w 4 the\_path\_of\_the\_file"

\input{result_size}
\input{result_duration}
