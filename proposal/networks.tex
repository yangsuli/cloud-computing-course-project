\section{Network Performance Measurements}
\label{section:networks} 

For network performance we concentrate on bandwidth and latency measurement; and use standard tools like iperf and ping. Both intra-cloud network and wide-area network performance are measured. We tried to catch the variation of network performance by performing our measures multiple times over a time span. The results reported here are three runs over a two-hour time span. Network performance variation over a longer period would be interesting to study too, but we leave that for future work due to time constraints.

\subsection{Intra-cloud network}
\label{subsection:intracloud}

Typically application hosted on a cloud will launch multiple instances, and have them communicate with each other. Thus intra-cloud network bandwidth and latency may greatly impact application performance; and would be interesting to study and understand. Typically a public cloud offering is divided into different zones, and user may choose in which zone to launch their instance. User may choose to launch instances in the same zone, to improve communication bandwidth and latency; or in different zones, to protect against correlated failure and improve availability. 

To measure intra-cloud network performance, we launched a server and two clients instances in Amazon EC2. One client is within the same zone as the server (us-east-1d), and the other client is in a different zone (us-east-1a). All of the three VMs are in the same 10.0.0.0/8 private subnet. For Microsoft Azure offering, we have a similar setup, but Azure doesn't have different zones. Instead they offer the option of connecting one VM to another in order to share workload. So we have one client connected to the server, and the other one not. Also, one Azure client is in a different subnet than the server, and could only access it through public virtual IP. In Table \ref{table:intra-band} we reported the results of three runs in two hours.

For network bandwidth, we use iperf to transfer as much data as possible between server and client in 10 seconds, using TCP protocol; and the results are reported in Table \ref{table:intra-band}. For network latency, we use ping to measure the round trip time between server and client. Because one Azure client is in a different subset of the others, it has to access other Azure instances using public IP. However traffic leaving the data center goes through the load balancer, and load balancer only routes TCP-based traffic. As ping uses ICMP protocol instead of TCP, this measurement is not possible to conduct between two unconnected Azure VMs. Thus we only report the Amazon EC2 results, and ping latency of two connected instances in Azure. In Table \ref{table:intra-latency} the round time latency measured using ping are reported. For each run, the number reported is an average of 3 or more 64 bytes package round trip time, which is of very small variation within a run.

\begin{table*}
\center
  \begin{tabular}{| l | l | l | l |}
\hline
Clinet Type & \multicolumn{3}{|c|}{Client to Server Bandwidth} \\
\hline      
& run1 & run2 & run3 \\
\hline
EC2 client (same zone) & 388MB/sec & 368MB/sec & 383MB/sec \\
EC2 client (different zone) & 901MB/sec & 905MB/sec & 906MB/sec \\
AZure client (connected to server) & 593MB/sec & 603MB/sec & 639MB/sec \\
AZure client (not connected to server) & 446MB/sec & 410MB/sec & 449MB/sec \\ 
\hline
\end{tabular}
\caption{Intra-cloud network bandwidth measurement}
\label{table:intra-band}
\end{table*}


\begin{table*}
\center
  \begin{tabular}{| l | l | l | l |}
\hline
Clinet Type & \multicolumn{3}{|c|}{Client to Server Latency} \\
\hline
& run1 & run2 & run3 \\
\hline
EC2 client (same zone) & 0.702ms & 0.649ms & 0.615ms \\
EC2 client (different zone) & 1.787ms & 1.788ms & 1.795ms \\
AZure client (connected to server) & 1.203ms & 1.978ms & 1.254ms \\
AZure client (not connected to server) & N/A & N/A & N/A \\ 
\hline
\end{tabular}
\caption{Intra-cloud network latency measurement}
\label{table:intra-latency}
\end{table*}

From those results we can make several observations. First, intra-cloud network is typically fast and of high bandwidth. Both latency and bandwidth are better than an order of magnitude compared with wide-erea network, whose performance we will discuss in \ref{subsection:widearea}. However, we do see some significant variation in intra-cloud network performance, especially in Microsoft Azure case. Azure also enforces very strict network policy of only allowing TCP traffic, which may impose some limit on applications. This problem might be mitigated by using Azure Connect though. Due to lack of knowledge on the underlying physical network topology, network performance is not always predictable; and sometimes we might see surprising performance characteristics: such as in Amazon EC2, inter-zone traffic has 3 times as much bandwidth as the intra-zone traffic in our measurement. Applications deployed on the cloud should be ready to cope with those unexpected and variable network behavior


\subsection{Wide-area network}
\label{subsection:widearea}
To quantify the wide-area network performance of public cloud offerings, we measure the bandwidth of the connection of cloud instances and mumble-01, a Core 2 Quad 2.66GHz, 8GB RAM machine running 64-bit RedHat 6 Linux system in the Computer Science Instructional Lab of University Wisconsin-Madison. This computer is connected via Gigabit Ethenet to a switch which has a Gigabit Ethenet link to the core network of UW-Madison. Latency is measured in a similar machine (ping.cs.wisc.edu) in the same lab.

We would like to point out that in order to throughoutly evaluate the wide-erea network performance, one needs to investage connectivity to muliple points in representative network locations. Our study in limited to one point due to time constraints, and the results are only preliminary and should be used with caution.

In Table \ref{table:inter-latency} we report the ping latency of there EC2 instances (small, medium and large) to ping.cs.wisc.edu. Again such measuremnt is not possible on Azure platform due to strict network traffic policy. The bandwidth between virtual instances and mumble-01 is reported in Table \ref{table:inter-band}.

\begin{table*}
\center
  \begin{tabular} {| l | l | l| l |}
\hline
Clinet Type & \multicolumn{3}{|c|}{Client to Server Latency} \\
\hline
& run1 & run2 & run3 \\
\hline
EC2 small instance & 34.022ms & 79.732ms & 34.033ms\\
EC2 medium instance & 34.099ms & 34.114ms & 33.891ms \\
EC2 large instance & 33.969ms & 34.299ms & 33.790ms \\
\hline
\end{tabular}
\caption{Wide-erea network latency measurement}
\label{table:inter-latency}
\end{table*}

\begin{table*}
\center
  \begin{tabular} {| l | l | l| l |}
\hline
Clinet Type & \multicolumn{3}{|c|}{Client to Server Bandwidth} \\
\hline
& run1 & run2 & run3 \\
\hline
EC2 small instance &  59.5 MB/s & 27.9 MB/s & 53.3 MB/s\\
EC2 medium instance & 178 MB/s & 33.9 MB/s & 291 MB/s \\
EC2 large instance & 138 MB/s & 159 MB/s & 140 MB/s \\
Azure small instance 1 & 39.6 MB/s & 44.0 MB/s & 48.8 MB/s\\
Azure small instance 2 & 29.0 MB/s & 37.3 MB/s & 18.4 MB/s\\
\hline
\end{tabular}
\caption{Wide-erea network bandwidth measurement}
\label{table:inter-band}
\end{table*}

From these results we could see that for EC2 instances, there is a tendency that with bigger instance you get less variant network performance. However, this needs to be confirmed with further experiments. Also, EC2 instance's wide-area networkk performance is consistently better than Azure, sometimes even in an order of magnitude.

In general, application hosted in the cloud should expect to deal with greater network bandwidth and latency variations.
